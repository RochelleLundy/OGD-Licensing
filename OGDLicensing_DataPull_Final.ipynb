{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA IMPORT AND CLEANING/FORMATTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_URI = \"http://api.us.socrata.com/api/catalog/v1?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of city open data portal domains\n",
    "domain_list = ['data.sfgov.org', 'opendata.lasvegasnevada.gov', 'data.cityofnewyork.us', 'data.cityofchicago.org', \n",
    "               'data.austintexas.gov', 'data.lacity.org', 'data.smgov.net', 'data.muni.org', 'data.brla.gov',\n",
    "              'data.seattle.gov', 'data.chattlibrary.org', 'data.vbgov.com', 'data.providenceri.gov', \n",
    "               'www.dallasopendata.com', 'data.fortworthtexas.gov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_socrata_data(city_domain):\n",
    "    \"\"\"Requests Socrata metadata on datasets at input domain name,\n",
    "    converts the response to a dictionary, and returns the dictionary.\n",
    "    \"\"\"\n",
    "    params = {'domains': city_domain, 'limit': 10000}\n",
    "    socrata_response = requests.get(discovery_URI, params = params)\n",
    "    # convert response from JSON string to Python list\n",
    "    metadata_list = socrata_response.json()\n",
    "    return metadata_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_city_list(city_result):\n",
    "    \"\"\"For each dataset metadata item in input list of API results, extracts relevant fields, creates\n",
    "    dictionary of values, appends dictionary to list, and returns the list.\n",
    "    \"\"\"\n",
    "    city_list = []\n",
    "    datasets = city_result['resultSetSize']\n",
    "    for d in range(datasets):\n",
    "            data_dict = {}\n",
    "            data_dict['num_city_datasets'] = city_result['resultSetSize']\n",
    "            data_dict['categories'] = city_result['results'][d]['classification']['categories']\n",
    "            if 'domain_category' in city_result['results'][d]['classification'].keys():\n",
    "                data_dict['domain_category'] = city_result['results'][d]['classification']['domain_category']\n",
    "            else:\n",
    "                data_dict['domain_category'] = 'None'\n",
    "            data_dict['domain'] = city_result['results'][d]['metadata']['domain']\n",
    "            data_dict['name'] = city_result['results'][d]['resource']['name']\n",
    "            data_dict['id'] = city_result['results'][d]['resource']['id']\n",
    "            data_dict['createdAt'] = city_result['results'][d]['resource']['createdAt']\n",
    "            data_dict['page_views_total'] = city_result['results'][d]['resource']['page_views']['page_views_total']\n",
    "            data_dict['download_count'] = city_result['results'][d]['resource']['download_count']\n",
    "            data_dict['type'] = city_result['results'][d]['resource']['type']\n",
    "            data_dict['provenance'] = city_result['results'][d]['resource']['provenance']\n",
    "            # datasets without a license lack a license field in the Socrata metadata\n",
    "            if 'license' in city_result['results'][d]['metadata'].keys():\n",
    "                data_dict['license'] = city_result['results'][d]['metadata']['license']\n",
    "            else:\n",
    "                data_dict['license'] = 'None'\n",
    "            city_list.append(data_dict)\n",
    "    return city_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_city_datasets(domain_list):\n",
    "    \"\"\"For each domain in input list, retrieves Socrata metadata on datasets at input domain name,\n",
    "    converts the response to a dictionary, extracts relevant fields for each dataset metadata item,\n",
    "    and creates a list of dictionaries for all dataset metadata items across domains.\n",
    "    \"\"\"\n",
    "    master_list = []\n",
    "    for dom in domain_list:\n",
    "        dom_result = get_socrata_data(dom)\n",
    "        dom_list = create_city_list(dom_result)\n",
    "        master_list = master_list + dom_list\n",
    "    return master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create master list of dictionaries of dataset metadata across all domains\n",
    "cities = all_city_datasets(domain_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert master list into dataframe\n",
    "cities_df = pd.DataFrame(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert createdAt column to datetime format\n",
    "cities_df['datetime_created'] = pd.to_datetime(cities_df['createdAt'])\n",
    "\n",
    "# create column for year created\n",
    "cities_df['year_created'] = cities_df.datetime_created.dt.year\n",
    "\n",
    "# create column for month created\n",
    "cities_df['month_created'] = cities_df.datetime_created.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of city name, population, and Open Data Census rank\n",
    "domain_dict = {'data.sfgov.org': ['San Francisco', 870887, 1], 'opendata.lasvegasnevada.gov':['Las Vegas', 632912, 2],\n",
    "               'data.cityofnewyork.us':['New York City', 8537673, 3], 'data.cityofchicago.org':['Chicago', 2704958, 4], \n",
    "               'data.austintexas.gov':['Austin', 947890, 5], 'data.lacity.org':['Los Angeles', 3976322, 6], \n",
    "               'data.smgov.net':['Santa Monica', 92478, 7], 'data.muni.org':['Anchorage', 298192, 10], \n",
    "               'data.brla.gov':['Baton Rouge', 227715, 11], 'data.seattle.gov':['Seattle', 704352, 12], \n",
    "               'data.chattlibrary.org':['Chattanooga', 177571, 14], 'data.vbgov.com':['Virginia Beach', 452602, 16], \n",
    "               'data.providenceri.gov':['Providence', 179219, 19], 'www.dallasopendata.com':['Dallas', 1317929, 35], \n",
    "               'data.fortworthtexas.gov':['Fort Worth', 854113, 0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns for city name and city population\n",
    "cities_df[\"city_info\"] = cities_df[\"domain\"].map(domain_dict)\n",
    "cities_df[\"city_name\"] = [x[0] for x in cities_df[\"city_info\"]]\n",
    "cities_df[\"city_population\"] = [x[1] for x in cities_df[\"city_info\"]]\n",
    "cities_df[\"city_census_rank\"] = [x[2] for x in cities_df[\"city_info\"]]\n",
    "# remove combined column\n",
    "cities_df = cities_df.drop('city_info', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataframe column names\n",
    "cities_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categories column\n",
    "cat_dummies = cities_df.categories.str.join('|').str.get_dummies().add_prefix('cat_')\n",
    "cat_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join category dummy variable columns to dataframe\n",
    "cities_df = cities_df.join(cat_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df['pageviews_by_pop'] = (cities_df['page_views_total']/cities_df['city_population'])*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df['downloads_by_pop'] = (cities_df['download_count']/cities_df['city_population'])*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df['downloads_by_pop'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine range of page_views & download counts\n",
    "city_grp = cities_df.groupby('city_name')\n",
    "city_grp[['download_count', 'page_views_total']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of dataset metadata records in dataframe \n",
    "len(cities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check license types \n",
    "cities_df.license.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary that categorizes licenses by rights type \n",
    "rights_dict = {'Open Data Commons Public Domain Dedication and License': 'Public Domain', \n",
    "               'Public Domain U.S. Government': 'Public Domain', 'None': 'None', \n",
    "               'Open Data Commons Open Database License': 'Attribution_Sharealike', 'Public Domain': 'Public Domain',\n",
    "               'Creative Commons Attribution 4.0 International': 'Attribution', 'Open Database License': 'Attribution_Sharealike',\n",
    "               'Creative Commons 1.0 Universal (Public Domain Dedication)': 'Public Domain',\n",
    "               'See Terms of Use': 'None', 'Open Data Commons Attribution License': 'Attribution'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create license type column in dataframe by mapping license types from dictionary\n",
    "cities_df[\"license_type\"] = cities_df[\"license\"].map(rights_dict)\n",
    "cities_df.license_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of datasets without license specified\n",
    "len(cities_df[cities_df.license=='None'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "license_groups = cities_df.groupby('license_type').size()\n",
    "type_dict = license_groups.T.to_dict()\n",
    "type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart of license type frequency\n",
    "license_groups.plot.bar(color = 'b', edgecolor = 'black')\n",
    "plt.xlabel('License Type')\n",
    "plt.ylabel('Number')\n",
    "plt.title('Counts of Licenses by Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.core.properties import value\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic_types = list(type_dict.keys())\n",
    "type_counts = list(type_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bokeh bar chart of license type frequency\n",
    "from bokeh.palettes import Viridis4\n",
    "source = ColumnDataSource(data=dict(lic_types=lic_types, counts=type_counts, color=Viridis4))\n",
    "\n",
    "p = figure(x_range=lic_types, plot_height=250, y_range=(0, 6000), title=\"License Type Counts\", toolbar_location = \"above\")\n",
    "p.vbar(x='lic_types', top='counts', width=0.9, color='color', legend=\"lic_types\", source=source)\n",
    "\n",
    "p.xgrid.grid_line_color = None\n",
    "p.legend.orientation = \"vertical\"\n",
    "p.legend.location = \"top_left\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic bar chart of license type counts adapted from Bokeh example\n",
    "\n",
    "# Here is a list of categorical values (or factors)\n",
    "types = list(type_dict.keys())\n",
    "counts = list(type_dict.values())\n",
    "\n",
    "# Set the x_range to the list of categories above\n",
    "p = figure(x_range=types, plot_height=250, title=\"License Type Counts\")\n",
    "\n",
    "# Categorical values can also be used as coordinates\n",
    "p.vbar(x=types, top=counts, width=0.9)\n",
    "\n",
    "# Set some properties to make the plot look better\n",
    "p.xgrid.grid_line_color = None\n",
    "p.y_range.start = 0\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked bar chart of license types grouped by city\n",
    "## do this as a percentage - i.e. license type %\n",
    "\n",
    "fig2 = (cities_df\n",
    " .groupby(['city_name', 'license_type'])\n",
    " .size()\n",
    " .unstack()\n",
    " .plot.bar(stacked=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import altair\n",
    "import altair as alt\n",
    "# enable notebook display\n",
    "alt.renderers.enable('notebook')\n",
    "# increase max rows size of source data\n",
    "alt.data_transformers.enable('default', max_rows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altair basic license type frequency bar chart\n",
    "## SEE EXAMPLE BELOW RE HOW TO SET COLORS\n",
    "alt.Chart(cities_df, width = 300).mark_bar(size = 40).encode(\n",
    "    x = 'license_type',\n",
    "    y = 'count()',\n",
    "    color = alt.Color('license_type', legend=alt.Legend(title='License Type'))\n",
    ").configure_bar(opacity = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altair stacked bar chart - license type frequencies by city\n",
    "alt.Chart(cities_df).mark_bar().encode(\n",
    "    x = 'city_name:N',\n",
    "    y = 'count(license_type):Q',\n",
    "    color = alt.Color('license_type', legend=alt.Legend(title='License Type'), scale=alt.Scale(range=['#440154', '#30678D', '#35B778', '#FDE724'])),\n",
    "    order = alt.Order('license_type', sort = 'ascending')\n",
    ").configure_bar(opacity = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altair stacked bar chart - license type frequencies by city - Normalized \n",
    "alt.Chart(cities_df, height = 300, width = 500).mark_bar().encode(\n",
    "    alt.X('city_name:N', axis = alt.Axis(title = 'City Name')),\n",
    "    alt.Y('count(license_type):Q', stack = 'normalize', axis=alt.Axis(title='Percentage of Datasets', format = '%')),\n",
    "    color = alt.Color('license_type', legend=alt.Legend(title='License Type'), scale=alt.Scale(range=['#440154', '#30678D', '#35B778', '#FDE724'])),\n",
    "    order = alt.Order('license_type', sort = 'ascending')\n",
    ").properties(title = \"License Type Distribution by City\").configure_legend(labelFontSize = 14).configure_axis(labelFontSize = 14, titleFontSize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather()\n",
    "\n",
    "alt.Chart(weather).mark_bar().encode(\n",
    "    alt.Color('weather:N',\n",
    "        legend=alt.Legend(title='Weather type'),\n",
    "        scale=alt.Scale(\n",
    "            domain=['sun', 'fog', 'drizzle', 'rain', 'snow'],\n",
    "            range=['#e7ba42', '#c7c7c7', '#aec7e8', '#1f77b4', '#9467bd']\n",
    "        ),\n",
    "    ),\n",
    "    alt.X('date:N',\n",
    "        axis=alt.Axis(title='Month of the Year'),\n",
    "        timeUnit='month',\n",
    "    ),\n",
    "    y='count()',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# license types by provenance - official vs community\n",
    "(cities_df\n",
    " .groupby(['provenance', 'license_type'])\n",
    " .size()\n",
    " .unstack()\n",
    " .plot.bar(stacked=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altair stacked bar chart - license type frequencies by provenance\n",
    "alt.Chart(cities_df, height = 300, width = 500).mark_bar().encode(\n",
    "    alt.X('provenance:N', axis = alt.Axis(title = 'Provenance')),\n",
    "    alt.Y('count(license_type):Q', axis=alt.Axis(title='Number of Datasets')),\n",
    "    color = alt.Color('license_type', legend=alt.Legend(title='License Type'), scale=alt.Scale(range=['#440154', '#30678D', '#35B778', '#FDE724'])),\n",
    "    order = alt.Order('license_type', sort = 'ascending')\n",
    ").properties(title = \"License Type Counts by Provenance\").configure_legend(labelFontSize = 14).configure_axis(labelFontSize = 14, titleFontSize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# license types by year dataset published\n",
    "(cities_df\n",
    " .groupby(['year_created', 'license_type'])\n",
    " .size()\n",
    " .unstack()\n",
    " .plot.bar(stacked=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_groups = cities_df.groupby('year_created').size()\n",
    "# bar chart of number of datasets published by year\n",
    "year_groups.plot.bar(color = 'b', edgecolor = 'black')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number Published')\n",
    "plt.title('Counts of Datasets Published by Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots for license type by pageviews and download counts\n",
    "import numpy as np\n",
    "\n",
    "log_download_count = cities_df['download_count'].apply(np.log)\n",
    "log_pageviews = cities_df['page_views_total'].apply(np.log)\n",
    "\n",
    "box_cities_df = pd.DataFrame({'log_downloadcount':log_download_count, 'log_pageviews': log_pageviews})\n",
    "box_cities_df['license_type'] = cities_df['license_type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots for license type by pageviews and download counts\n",
    "#plt.style.use('seaborn-poster')\n",
    "\n",
    "box_cities_df.boxplot(column = 'log_pageviews', by = 'license_type', fontsize = '12', rot = 45)\n",
    "plt.title('Boxplot - Log Pageviews Grouped by License Type')\n",
    "plt.ylabel('Log Pageviews')\n",
    "plt.xlabel('License Type')\n",
    "plt.suptitle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_cities_df.boxplot(column = 'log_downloadcount', by = 'license_type', fontsize = '10', rot = 45)\n",
    "plt.title('Boxplot - Log Download Count Grouped by License Type')\n",
    "plt.ylabel('Log Download Count')\n",
    "plt.xlabel('License Type')\n",
    "plt.suptitle('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of datasets by city population size\n",
    "p = figure(title = \"Number of Published Datasets by City Population Size\")\n",
    "p.xaxis.axis_label = 'City Population'\n",
    "p.yaxis.axis_label = 'Number of Datasets'\n",
    "\n",
    "p.square(cities_df[\"city_population\"], cities_df[\"num_city_datasets\"],\n",
    "         color='blue', fill_alpha=0.4, size=8)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTIVE MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score, train_test_split \n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to print metrics for model predictions\n",
    "def printScores(labels, predictions):\n",
    "    \"\"\"Prints accuracy score, confusion matrix, and classification report containing\n",
    "    precision, recall and f-score measures comparing input class predictions with \n",
    "    input actual labels.\n",
    "    \"\"\"\n",
    "    print('Accuracy:', accuracy_score(labels, predictions), '\\n')\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(labels, predictions))\n",
    "    print('(Row = Actual, Column = Predicted)', '\\n')\n",
    "    print('Classification Report:\\n', classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE DATA FOR USE IN TREE-BASED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not needed for predictive modeling\n",
    "pred_df = cities_df.drop(['categories', 'createdAt', 'domain', 'domain_category', 'id', 'license',\n",
    "                         'name', 'datetime_created', 'city_name', 'download_count', 'page_views_total'], axis = 1)\n",
    "\n",
    "pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical variables\n",
    "le = preprocessing.LabelEncoder()\n",
    "pred_df['type'] = le.fit_transform(pred_df['type'].astype(str))\n",
    "pred_df['provenance'] = le.fit_transform(pred_df['provenance'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing download_count values with out-of-range value \n",
    "#pred_df['download_count'] = pred_df.download_count.fillna(-100)\n",
    "pred_df['downloads_by_pop'] = pred_df.downloads_by_pop.fillna(-100)\n",
    "\n",
    "# recheck for missing values\n",
    "pred_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic_df = pred_df[['license_type', 'city_census_rank', 'city_population', 'type', 'provenance','year_created','downloads_by_pop', 'pageviews_by_pop', 'cat_public safety']]\n",
    "lic_df = shuffle(lic_df)\n",
    "lic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATASET ONE - RESTRICTIONS (Database + Attribution), PUBLIC DOMAIN, NONE\n",
    "# USE CLASS_WEIGHT = 'BALANCED' PARAMETER IN MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe variable\n",
    "ds1_pred_df = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to re-map license types \n",
    "ds1_rights_dict = {'Public Domain':'Public Domain', 'None': 'None', \n",
    "               'Attribution':'Attribution_SA', 'Attribution_Sharealike':'Attribution_SA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new license type column in dataframe by mapping license types from dictionary\n",
    "ds1_pred_df[\"ds1_license_type\"] = ds1_pred_df[\"license_type\"].map(ds1_rights_dict)\n",
    "ds1_pred_df.ds1_license_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original license_type column \n",
    "ds1_pred_df = ds1_pred_df.drop(['license_type'], axis = 1)\n",
    "\n",
    "ds1_pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle dataframe\n",
    "ds1_pred_df = shuffle(ds1_pred_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate target variable\n",
    "ds1_license_label = ds1_pred_df.pop('ds1_license_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "ds1_pred_train, ds1_pred_test, ds1_lbl_train, ds1_lbl_test = train_test_split(ds1_pred_df, ds1_license_label, test_size = 0.2)\n",
    "\n",
    "print(len(ds1_pred_train))\n",
    "print(len(ds1_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-level features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-level features only\n",
    "# drop city-level features from training and test sets\n",
    "ds1_nocity_train = ds1_pred_train.drop(['city_population', 'num_city_datasets', 'city_census_rank'], axis = 1)\n",
    "\n",
    "ds1_nocity_test = ds1_pred_test.drop(['city_population', 'num_city_datasets', 'city_census_rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DECISION TREE - ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree model\n",
    "DecTree1 = tree.DecisionTreeClassifier(criterion = 'entropy', class_weight = 'balanced')\n",
    "\n",
    "# fit model with training data\n",
    "DecTree1.fit(ds1_pred_train, ds1_lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "dt1_tr_predict = DecTree1.predict(ds1_pred_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('Decision Tree Scores on Training Data:\\n')\n",
    "printScores(ds1_lbl_train, dt1_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "dt1_test_predict = DecTree1.predict(ds1_pred_test)\n",
    "\n",
    "# print scores for test data\n",
    "print('Decision Tree Scores on Test Data:\\n')\n",
    "printScores(ds1_lbl_test, dt1_test_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST - ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest classifer using 10 trees and entropy as split criterion\n",
    "RandFor1 = RandomForestClassifier(criterion = 'entropy', class_weight = 'balanced', n_estimators = 40)\n",
    "\n",
    "# fit model with training data\n",
    "RandFor1.fit(ds1_pred_train, ds1_lbl_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "rf1_tr_predict = RandFor1.predict(ds1_pred_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('Random Forest Scores on Training Data - 10 Trees:\\n')\n",
    "printScores(ds1_lbl_train, rf1_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "rf1_tst_predict = RandFor1.predict(ds1_pred_test)\n",
    "    \n",
    "# print scores for test data\n",
    "print('Random Forest Scores on Test Data - 10 Trees:\\n')\n",
    "printScores(ds1_lbl_test, rf1_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRATREES - ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create extremely randomized trees classifer using 10 trees and entropy as split criterion\n",
    "ExTree1 = ExtraTreesClassifier(criterion = 'entropy', class_weight = 'balanced', n_estimators = 40)\n",
    "\n",
    "# fit model with training data\n",
    "ExTree1.fit(ds1_pred_train, ds1_lbl_train)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "ex1_tr_predict = ExTree1.predict(ds1_pred_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('ExtraTrees Scores on Training Data - 10 Trees:\\n')\n",
    "printScores(ds1_lbl_train, ex1_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "ex1_tst_predict = ExTree1.predict(ds1_pred_test)\n",
    "    \n",
    "# print scores for test data\n",
    "print('ExtraTrees Scores on Test Data - 10 Trees:\\n')\n",
    "printScores(ds1_lbl_test, ex1_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCES OF BEST MODEL\n",
    "# best model for Dataset 1 - ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view importance of predictor variables in ExtraTrees model\n",
    "ds1_feat_import = ExTree1.feature_importances_\n",
    "ds1_pred_vars = list(ds1_pred_df.columns)\n",
    "ds1_feats_ranked = list(zip(ds1_pred_vars, ds1_feat_import))\n",
    "ds1_feats_ranked.sort(key = lambda tup: tup[1], reverse = True) \n",
    "ds1_feats_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "importances = ExTree1.feature_importances_\n",
    "std = np.std([ExTree1.feature_importances_ for tree in ExTree1.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "fig1 = plt.figure(figsize = (10, 10))\n",
    "plt.title(\"3-Class Dataset - Extremely Randomized Trees - Feature Importances\", fontsize = 16)\n",
    "plt.barh(range(ds1_pred_train.shape[1]), importances[indices],\n",
    "       color=\"#29788E\", yerr=std[indices], align=\"center\")\n",
    "plt.yticks(range(ds1_pred_train.shape[1]), [ds1_pred_vars[i] for i in indices], fontsize = 14)\n",
    "plt.ylim([-1, ds1_pred_train.shape[1]])\n",
    "plt.show()\n",
    "fig1.savefig('fig1.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST MODEL ON DATASET-LEVEL ONLY FEATURES\n",
    "# best model for Dataset 1 - ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create extremely randomized trees classifer using 10 trees and entropy as split criterion\n",
    "ExTree1A = ExtraTreesClassifier(criterion = 'entropy', class_weight = 'balanced')\n",
    "\n",
    "# fit model with training data\n",
    "ExTree1A.fit(ds1_nocity_train, ds1_lbl_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "ex1A_tr_predict = ExTree1A.predict(ds1_nocity_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('ExtraTrees Scores on Dataset-Only Features Training Data - 10 Trees:\\n')\n",
    "printScores(ds1_lbl_train, ex1A_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "ex1A_tst_predict = ExTree1A.predict(ds1_nocity_test)\n",
    "    \n",
    "# print scores for test data\n",
    "print('ExtraTrees Scores on Test Data - 10 Trees:\\n')\n",
    "printScores(ds1_lbl_test, ex1A_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view importance of predictor variables in ExtraTrees Dataset-level only model\n",
    "ds1NC_feat_import = ExTree1A.feature_importances_\n",
    "ds1NC_pred_vars = list(ds1_nocity_train.columns)\n",
    "ds1NC_feats_ranked = list(zip(ds1NC_pred_vars, ds1NC_feat_import))\n",
    "ds1NC_feats_ranked.sort(key = lambda tup: tup[1], reverse = True) \n",
    "ds1NC_feats_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "importances = ExTree1A.feature_importances_\n",
    "std = np.std([ExTree1A.feature_importances_ for tree in ExTree1A.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "fig1A = plt.figure(figsize = (10, 10))\n",
    "plt.title(\"3-Class Dataset - Extremely Randomized Trees - Dataset-Level Feature Importances\", fontsize = 16)\n",
    "plt.barh(range(ds1_nocity_train.shape[1]), importances[indices],\n",
    "       color=\"#FDE724\", yerr=std[indices], align=\"center\")\n",
    "plt.yticks(range(ds1_nocity_train.shape[1]), [ds1NC_pred_vars[i] for i in indices], fontsize = 14)\n",
    "plt.ylim([-1, ds1_nocity_train.shape[1]])\n",
    "plt.show()\n",
    "fig1A.savefig('fig1A.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATASET TWO - LICENSED (Database + Attribution + Public Domain), NO LICENSE (None)\n",
    "# check how closely classes balance - ****REMOVE CLASS WEIGHT BALANCE PARAMETER???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe variable\n",
    "ds2_pred_df = pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to re-map license types \n",
    "ds2_rights_dict = {'Public Domain':'License', 'None': 'No_License', \n",
    "               'Attribution':'License', 'Attribution_Sharealike':'License'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new license type column in dataframe by mapping license types from dictionary\n",
    "ds2_pred_df[\"ds2_license_type\"] = ds2_pred_df[\"license_type\"].map(ds2_rights_dict)\n",
    "ds2_pred_df.ds2_license_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class balance\n",
    "### CONSIDER SAMPLING THESE TO BALANCE EXACTLY ###\n",
    "ds2_pred_df.groupby('ds2_license_type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original license_type column \n",
    "ds2_pred_df = ds2_pred_df.drop(['license_type'], axis = 1)\n",
    "\n",
    "ds2_pred_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle dataframe\n",
    "ds2_pred_df = shuffle(ds2_pred_df).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate target variable\n",
    "ds2_license_label = ds2_pred_df.pop('ds2_license_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "ds2_pred_train, ds2_pred_test, ds2_lbl_train, ds2_lbl_test = train_test_split(ds2_pred_df, ds2_license_label, test_size = 0.2)\n",
    "\n",
    "print(len(ds2_pred_train))\n",
    "print(len(ds2_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset-level features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop city-level features from training and test sets\n",
    "ds2_nocity_train = ds2_pred_train.drop(['city_population', 'num_city_datasets', 'city_census_rank'], axis = 1)\n",
    "\n",
    "ds2_nocity_test = ds2_pred_test.drop(['city_population', 'num_city_datasets', 'city_census_rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DECISION TREE - ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree model\n",
    "DecTree2 = tree.DecisionTreeClassifier(criterion = 'entropy', class_weight = 'balanced')\n",
    "\n",
    "# fit model with training data\n",
    "DecTree2.fit(ds2_pred_train, ds2_lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "dt2_tr_predict = DecTree2.predict(ds2_pred_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('Decision Tree Scores on Training Data:\\n')\n",
    "printScores(ds2_lbl_train, dt2_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "dt2_test_predict = DecTree2.predict(ds2_pred_test)\n",
    "\n",
    "# print scores for test data\n",
    "print('Decision Tree Scores on Test Data:\\n')\n",
    "printScores(ds2_lbl_test, dt2_test_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RANDOM FOREST - ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest classifer using 10 trees and entropy as split criterion\n",
    "RandFor2 = RandomForestClassifier(criterion = 'entropy', class_weight = 'balanced', n_estimators = 40)\n",
    "\n",
    "# fit model with training data\n",
    "RandFor2.fit(ds2_pred_train, ds2_lbl_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "rf2_tr_predict = RandFor2.predict(ds2_pred_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('Random Forest Scores on Training Data - 10 Trees:\\n')\n",
    "printScores(ds2_lbl_train, rf2_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "rf2_tst_predict = RandFor2.predict(ds2_pred_test)\n",
    "    \n",
    "# print scores for test data\n",
    "print('Random Forest Scores on Test Data - 10 Trees:\\n')\n",
    "printScores(ds2_lbl_test, rf2_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTRATREES - ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create extremely randomized trees classifer using 10 trees and entropy as split criterion\n",
    "ExTree2 = ExtraTreesClassifier(criterion = 'entropy', class_weight = 'balanced', n_estimators = 40)\n",
    "\n",
    "# fit model with training data\n",
    "ExTree2.fit(ds2_pred_train, ds2_lbl_train)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "ex2_tr_predict = ExTree2.predict(ds2_pred_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('ExtraTrees Scores on Training Data - 10 Trees:\\n')\n",
    "printScores(ds2_lbl_train, ex2_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "ex2_tst_predict = ExTree2.predict(ds2_pred_test)\n",
    "    \n",
    "# print scores for test data\n",
    "print('ExtraTrees Scores on Test Data - 10 Trees:\\n')\n",
    "printScores(ds2_lbl_test, ex2_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE IMPORTANCES OF BEST MODEL\n",
    "# best model for Dataset 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view importance of predictor variables in Random Forest model\n",
    "ds2_feat_import = RandFor2.feature_importances_\n",
    "ds2_pred_vars = list(ds2_pred_df.columns)\n",
    "ds2_feats_ranked = list(zip(ds2_pred_vars, ds2_feat_import))\n",
    "ds2_feats_ranked.sort(key = lambda tup: tup[1], reverse = True) \n",
    "ds2_feats_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "importances = RandFor2.feature_importances_\n",
    "std = np.std([RandFor2.feature_importances_ for tree in RandFor2.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "fig2 = plt.figure(figsize = (10, 10))\n",
    "plt.title(\"2-Class Dataset - Random Forest - Feature Importances\", fontsize = 16)\n",
    "plt.barh(range(ds2_pred_train.shape[1]), importances[indices],\n",
    "       color=\"#440154\", yerr=std[indices], align=\"center\")\n",
    "plt.yticks(range(ds2_pred_train.shape[1]), [ds2_pred_vars[i] for i in indices], fontsize = 14)\n",
    "plt.ylim([-1, ds2_pred_train.shape[1]])\n",
    "plt.show()\n",
    "fig2.savefig('fig2.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST MODEL ON DATASET-LEVEL ONLY FEATURES\n",
    "# best model for Dataset 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random forest classifer using 10 trees and entropy as split criterion\n",
    "RandFor2A = RandomForestClassifier(criterion = 'entropy', class_weight = 'balanced')\n",
    "\n",
    "# fit model with training data\n",
    "RandFor2A.fit(ds2_nocity_train, ds2_lbl_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on training data\n",
    "rf2A_tr_predict = RandFor2A.predict(ds2_nocity_train)\n",
    "\n",
    "# print scores for training data\n",
    "print('Random Forest Scores on Training Data - 10 Trees:\\n')\n",
    "printScores(ds2_lbl_train, rf2A_tr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "rf2A_tst_predict = RandFor2A.predict(ds2_nocity_test)\n",
    "    \n",
    "# print scores for test data\n",
    "print('Random Forest Scores on Test Data - 10 Trees:\\n')\n",
    "printScores(ds2_lbl_test, rf2A_tst_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view importance of predictor variables in Random Forest model\n",
    "ds2NC_feat_import = RandFor2A.feature_importances_\n",
    "ds2NC_pred_vars = list(ds2_nocity_train.columns)\n",
    "ds2NC_feats_ranked = list(zip(ds2NC_pred_vars, ds2NC_feat_import))\n",
    "ds2NC_feats_ranked.sort(key = lambda tup: tup[1], reverse = True) \n",
    "ds2NC_feats_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "importances = RandFor2A.feature_importances_\n",
    "std = np.std([RandFor2A.feature_importances_ for tree in RandFor2A.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "fig2A = plt.figure(figsize = (10, 10))\n",
    "plt.title(\"2-Class Dataset - Random Forest - Dataset-Level Feature Importances\", fontsize = 16)\n",
    "plt.barh(range(ds2_nocity_train.shape[1]), importances[indices],\n",
    "       color=\"#22A784\", yerr=std[indices], align=\"center\")\n",
    "plt.yticks(range(ds2_nocity_train.shape[1]), [ds2NC_pred_vars[i] for i in indices], fontsize = 14)\n",
    "plt.ylim([-1, ds2_nocity_train.shape[1]])\n",
    "plt.show()\n",
    "fig2A.savefig('fig2A.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
